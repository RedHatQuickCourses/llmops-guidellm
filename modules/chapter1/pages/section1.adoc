= Course: Model Performance Benchmarking with GuideLLM
:imagesdir: ../assets/images
:toc: left
:toclevels: 3
:sectnums:

// -- Page Break --

== Introduction to Performance Benchmarking

Welcome to the Model Performance Benchmarking with GuideLLM course. In this course, you will learn how to quantitatively measure, analyze, and optimize the performance of Large Language Models deployed on Red Hat OpenShift AI.

By the end of this course, you will be able to:
* Set up an automated benchmarking pipeline using GuideLLM and Tekton.
* Execute various performance tests that simulate real-world workloads.
* Interpret key performance metrics like latency, throughput, and token generation speed.
* Connect performance results to business outcomes, such as user experience and infrastructure cost.

=== Why Performance Evaluation Matters

In Generative AI systems, evaluating system performance including latency, throughput, and resource utilization is just as important as evaluating model accuracy or quality. Here's why:

. **User Experience**: High latency leads to sluggish interactions, which is unacceptable in chatbots, copilots, and real-time applications. Users expect sub-second responses.
. **Scalability**: Throughput determines how many requests a system can handle in parallel. For enterprise GenAI apps, high throughput is essential to serve multiple users or integrate with high-frequency backend processes.
. **Cost Efficiency**: Slow or inefficient systems require more compute to serve the same volume of requests. Optimizing performance directly reduces cloud GPU costs and improves ROI.
. **Fair Benchmarking**: A model may appear “better” in isolated evaluation, but if it requires excessive inference time or hardware, it may not be viable in production. True model evaluation must balance quality and performance.
. **Deployment Readiness**: Latency and throughput impact architectural decisions (e.g., batching, caching, distributed serving). Measuring them ensures a model is viable under real-world constraints.

=== What is GuideLLM?

**GuideLLM** is a toolkit for evaluating and optimizing the deployment of LLMs. By simulating real-world inference workloads, GuideLLM enables you to easily assess the performance, resource requirements, and cost implications of deploying LLMs on various hardware configurations. This approach ensures efficient, scalable, and cost-effective LLM inference serving while maintaining high service quality.

GuideLLM is now officially a part of the vLLM upstream project. This toolset is one of the primary ways Red Hat internal teams are benchmarking customer models and will be the main framework we will recommend to our customers.

NOTE: It's important to distinguish GuideLLM from Trusty AI. Trusty AI's scope is responsible AI (explainability, fairness, bias detection), while GuideLLM is focused purely on system performance benchmarking and optimization.

// -- Page Break --

== Module 1: Setting Up Your Benchmarking Environment

In this module, we will set up an automated Tekton pipeline on OpenShift AI to run our GuideLLM benchmarks. Using a Tekton pipeline provides automation, reproducibility, and efficient resource management for our tests.

=== 1.1 Install the Tekton CLI

First, ensure the Tekton CLI (`tkn`) is installed in your terminal. This tool will allow us to interact with our pipelines.

[source,console,role=execute,subs=attributes+]
----
curl -sL $(curl -s https://api.github.com/repos/tektoncd/cli/releases/latest | grep "browser_download_url.*_Linux_x86_64.tar.gz" | cut -d '"' -f 4) | sudo tar -xz -C /usr/local/bin tkn
tkn version
----

=== 1.2 Deploy the GuideLLM Pipeline Resources

Next, we will clone the necessary Git repositories and apply the Kubernetes resources that define our pipeline, its tasks, and the required storage.

. Clone the ETX vLLM optimization repo which contains the pipeline definition.
+
[source,console,role=execute,subs=attributes+]
----
git clone https://github.com/redhat-ai-services/etx-llm-optimization-and-inference-leveraging.git
cd etx-llm-optimization-and-inference-leveraging
----

. Clone the GuideLLM pipeline repo itself.
+
[source,console,role=execute,subs=attributes+]
----
git clone https://github.com/jhurlocker/guidellm-pipeline.git
----

. Apply the PVC, task, pipeline, and Minio bucket configuration to your cluster. Ensure you are in the correct namespace (`vllm`).
+
[source,console,role=execute,subs=attributes+]
----
oc apply -f guidellm-pipeline/pipeline/upload-results-task.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/guidellm-pipeline.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/pvc.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/guidellm-benchmark-task.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/mino-bucket.yaml -n vllm
----

With these resources created, we are now ready to execute a benchmark.

// -- Page Break --

== Module 2: Running Your First Benchmark

Now that the pipeline is set up, let's run our first test. This will help us establish a baseline and understand the end-to-end workflow.

=== 2.1 Prepare the Target Model

Before running the pipeline, ensure your target model is deployed and accessible. For this lab, we will use the `granite-8b` model.

. Get the inference endpoint URL for your deployed model.
+
[source,console,role=execute,subs=attributes+]
----
export INFERENCE_ENDPOINT=$(oc get inferenceservice granite-8b -n vllm -o jsonpath='{.status.url}')
echo $INFERENCE_ENDPOINT
----
NOTE: If you have not deployed the `granite-8b` model yet, run the following Helm command: `helm upgrade -i granite-8b redhat-ai-services/vllm-kserve --version 0.5.11 --values workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml -n vllm`

=== 2.2 Execute the Pipeline

Now, we will start the Tekton pipeline. We'll pass parameters to define the target model, the workload profile (`data-config`), and the test type (`rate-type`). We will use a `sweep` which automatically tests a range of request rates.

[source,console,role=execute,subs=attributes+]
----
tkn pipeline start guidellm-benchmark-pipeline -n vllm \
  --param target=$INFERENCE_ENDPOINT/v1 \
  --param model-name="granite-8b" \
  --param processor="ibm-granite/granite-3.3-8b-instruct" \
  --param data-config="prompt_tokens=512,output_tokens=128" \
  --param max-seconds="30" \
  --param huggingface-token="" \
  --param api-key="" \
  --param rate="2" \
  --param rate-type="sweep" \
  --param max-concurrency="10" \
  --workspace name=shared-workspace,claimName=guidellm-output-pvc
----

=== 2.3 Retrieve and View Results

The pipeline will run the benchmark and upload the results as a text file to a Minio S3 bucket.

. Get the route to the Minio UI. The login is `admin/admin123`.
+
[source,console,role=execute,subs=attributes+]
----
oc get route minio-ui -n ic-shared-minio -o jsonpath='{.spec.host}'
----

. Navigate to the Minio URL in your browser, log in, and find the `guidellm-benchmark` bucket.
+
image::benchmark_results_file_in_minio.png[benchmark results in Minio]

. Download the `benchmark-<TIMESTAMP>.txt` file and open it in a text editor to view the raw results.
+
image::benchmark_results_minio.png[benchmark results]

// -- Page Break --

== Module 3: Interpreting Benchmark Results

Understanding the output is crucial for drawing meaningful conclusions. This module breaks down the key metrics and how to interpret them.

=== 3.1 Key GuideLLM Metrics

GuideLLM captures several critical metrics. Here are the most important ones:

* **Requests per Second**: Total requests completed per second (Throughput).
* **Request concurrency**: The average number of requests being processed in parallel.
* **Output token per second (mean)**: The generation speed of the model.
* **Time to First Token (TTFT)**: How quickly the first token is returned. This is critical for user-perceived responsiveness. (Measured in ms: mean, median, p99).
* **Inter-Token Latency (ITL)**: The time between subsequent tokens in a response. This affects the smoothness of streaming. (Measured in ms: mean, median, p99).
* **Request latency**: Total end-to-end time for a request. (Measured in ms: mean, median, p99).

See the complete https://github.com/neuralmagic/guidellm/blob/main/docs/metrics.md[metrics documentation^].

=== 3.2 Reading the Output File

The output file has two main sections.

==== Top Section (Benchmark Info)
This section summarizes the test parameters.
* **Benchmark**: The type of benchmark that ran (e.g., `constant@2` indicates 2 requests per second).
* **Requests Made**: Total number of requests issued.
* **Token Data**: Average and total tokens processed.

==== Bottom Section (Benchmark Stats)
This section contains the performance statistics. When analyzing these, pay close attention to the distribution:

* **Mean**: The overall average. Good for a general performance overview.
* **Median (p50)**: The typical user experience. It is less skewed by a few very slow requests.
* **p99**: The 99th percentile, representing the worst-case latency for the vast majority of users. This is essential for defining and meeting Service Level Objectives (SLOs). A large gap between the median and p99 indicates inconsistent performance under load.

=== 3.3 The Importance of Data in Benchmarking

The data you use for testing dramatically impacts the results.

* **This training uses emulated data** (`{"type":"emulated","prompt_tokens":512,"output_tokens":128}`) for consistency and simplicity.
* **For client engagements, always use representative production data.** Real workloads differ significantly from synthetic data in token distribution and response variability. Using client data can reveal 25-40% higher latency variance.

**Key Takeaway**: Use stock data for learning and initial baselines; use client data for production recommendations.

// -- Page Break --

== Module 4: Advanced Benchmarking Scenarios

In this module, you will run a series of targeted tests to simulate common customer use cases and understand how different workloads impact performance.

=== 4.1 Exercise 1: Token Size Impact Analysis

Here, we'll test how different input/output token sizes, representing different applications, affect performance.

==== 4.1a: Chat Application Simulation (Short I/O)
Represents a typical conversational AI scenario.
[source,console,role=execute]
----
# data-config="prompt_tokens=256,output_tokens=128"
# (Execute the tkn pipeline start command with the above data-config)
----

==== 4.1b: RAG Simulation (Large Input)
Represents enterprise knowledge base queries with large context.
[source,console,role=execute]
----
# data-config="prompt_tokens=4096,output_tokens=512"
# (Execute the tkn pipeline start command with the above data-config)
----

==== 4.1c: Code Generation Workload (Balanced I/O)
Represents AI-powered developer tools.
[source,console,role=execute]
----
# data-config="prompt_tokens=512,output_tokens=512"
# (Execute the tkn pipeline start command with the above data-config)
----

**Analysis**: Compare the TTFT, output tokens/sec, and overall latency across these three runs. You will likely see that larger token counts significantly increase latency and reduce throughput.

=== 4.2 Exercise 2: Rate Type Deep Dive

Different `--rate-type` arguments reveal distinct performance characteristics.

==== 4.2a: Peak Capacity Assessment (`throughput`)
This test sends all requests in parallel to find the theoretical maximum capacity of the server.
[source,console,role=execute]
----
# rate-type="throughput"
# (Execute the tkn pipeline start command with the above rate-type)
----
**Consulting Value**: Establishes the upper bound for infrastructure sizing and identifies hardware bottlenecks.

==== 4.2b: Real-World Load Simulation (`constant`)
This test sends requests at a steady, sustained rate.
[source,console,role=execute]
----
# rate-type="constant"
# (Execute the tkn pipeline start command with the above rate-type and a specific --rate, e.g., --rate="4")
----
**Consulting Value**: Validates performance under realistic production loads and helps define SLAs.

==== 4.2c: Burst Traffic Analysis (`poisson`)
This test sends requests at an irregular, bursty rate, which models many real-world traffic patterns.
[source,console,role=execute]
----
# rate-type="poisson"
# (Execute the tkn pipeline start command with the above rate-type and a mean --rate)
----
**Consulting Value**: Reveals how well the system handles unpredictable traffic spikes and informs autoscaling configurations.

// -- Page Break --

== Course Wrap-up

Congratulations on completing the Model Performance Benchmarking course.

=== Summary of Learnings

In this course, we demonstrated how to evaluate system performance for a deployed Large Language Model using GuideLLM.

You learned to:
* Set up an automated testing environment using a **Tekton pipeline**.
* Run benchmarks using different **workload profiles** (token sizes) and **rate types** (throughput, constant, sweep) to simulate real-world scenarios.
* Retrieve and analyze key performance metrics, focusing on **latency (TTFT, ITL)** and **throughput**.
* Understand the critical difference between the **mean, median, and p99** percentiles for assessing user experience and defining SLOs.

By mastering these skills, you can better align model serving with application needs—whether you’re optimizing for cost, speed, or user experience—and provide data-driven recommendations that deliver significant value to customers.