<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Evaluating System Performance with GuideLLM :: Model Performance Benchmarking with GuideLLM</title>
    <link rel="prev" href="../LABENV/index.html">
    <link rel="next" href="mission.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Model Performance Benchmarking with GuideLLM</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-guidellm" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Model Performance Benchmarking with GuideLLM</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Evaluating System Performance with GuideLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section1.html">Introduction to Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section2.html">Module 2: Running Your First Benchmark</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">Module 3: Interpreting Benchmark Results</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section4.html">Module 4: Advanced Benchmarking Scenarios</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Model Performance Benchmarking with GuideLLM</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Model Performance Benchmarking with GuideLLM</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Model Performance Benchmarking with GuideLLM</a></li>
    <li><a href="index.html">Evaluating System Performance with GuideLLM</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Evaluating System Performance with GuideLLM</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In Generative AI systems, evaluating system performance including latency, throughput, and resource utilization is just as important as evaluating model accuracy or quality. Here&#8217;s why:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>User Experience</strong>: High latency leads to sluggish interactions, which is unacceptable in chatbots, copilots, and real-time applications. Users expect sub-second responses.</p>
</li>
<li>
<p><strong>Scalability</strong>: Throughput determines how many requests a system can handle in parallel. For enterprise GenAI apps, high throughput is essential to serve multiple users or integrate with high-frequency backend processes.</p>
</li>
<li>
<p><strong>Cost Efficiency</strong>: Slow or inefficient systems require more compute to serve the same volume of requests. Optimizing performance directly reduces cloud GPU costs and improves ROI.</p>
</li>
<li>
<p><strong>Fair Benchmarking</strong>: A model may appear “better” in isolated evaluation, but if it requires excessive inference time or hardware, it may not be viable in production. True model evaluation must balance quality and performance.</p>
</li>
<li>
<p><strong>Deployment Readiness</strong>: Latency and throughput impact architectural decisions (e.g., batching, caching, distributed serving). Measuring them ensures a model is viable under real-world constraints.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_is_guidellm"><a class="anchor" href="#_what_is_guidellm"></a>What is GuideLLM?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>GuideLLM is a toolkit for evaluating and optimizing the deployment of LLMs. By simulating real-world inference workloads, GuideLLM enables you to easily assess the performance, resource requirements, and cost implications of deploying LLMs on various hardware configurations. This approach ensures efficient, scalable, and cost-effective LLM inference serving while maintaining high service quality.</p>
</div>
<div class="paragraph">
<p>GuideLLM is now officially a part of the vLLM upstream project. This toolset is one of the primary ways Red Hat internal teams are benchmarking customer models. GuideLLM will be the main framework we will recommend to our customers with the scope of model performance and optimization.</p>
</div>
<div class="sect2">
<h3 id="_trusty_ai_vs_guidellm"><a class="anchor" href="#_trusty_ai_vs_guidellm"></a>Trusty AI vs GuideLLM</h3>
<div class="paragraph">
<p>Trusty AI maintains the scope of responsable AI, while GuideLLM is focused on benchmarking and model optimization. That being said, there are some current crossovers. Trusty AI incorporates lm-eval-harness and GuideLLM is roadmapped to include this test harness as well. Trusty AI will continue to be an incorporated and supported operator deployment in RHOAI. There are currently no plans to have a similar deployment method for GuideLLM.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_set_up_guidellm_tekton_pipeline"><a class="anchor" href="#_set_up_guidellm_tekton_pipeline"></a>Set Up GuideLLM tekton pipeline</h2>
<div class="sectionbody">
<div class="paragraph">
<p>There are several current ways you may deploy and use GuideLLM.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>CLI tool: documented in the upstream project.</p>
</li>
<li>
<p>Python library: Not yet in current upstream documentation. You can see an example here in this guide: <a href="https://redhatquickcourses.github.io/genai-vllm/genai-vllm/1/rhoai_deploy/guide_llm.html" class="bare">https://redhatquickcourses.github.io/genai-vllm/genai-vllm/1/rhoai_deploy/guide_llm.html</a>.</p>
</li>
<li>
<p>Kubernetes job: <a href="https://github.com/rh-aiservices-bu/guidellm-pipeline" class="bare">https://github.com/rh-aiservices-bu/guidellm-pipeline</a></p>
</li>
<li>
<p>Tekton pipeline: <a href="https://github.com/jhurlocker/guidellm-pipeline.git" class="bare">https://github.com/jhurlocker/guidellm-pipeline.git</a> (forked from above repo)</p>
<div class="ulist">
<ul>
<li>
<p>Run the below command to install the Tekton CLI</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">curl -sL $(curl -s https://api.github.com/repos/tektoncd/cli/releases/latest | grep "browser_download_url.*_Linux_x86_64.tar.gz" | cut -d '"' -f 4) | sudo tar -xz -C /usr/local/bin tkn
tkn version</code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>For our lab today, we will utilize the tekton pipeline on our OpenShift AI cluster. A pipeline deployment provides the following benefits:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Automation and reproducibility</p>
</li>
<li>
<p>Cloud-native / kubernetes-native</p>
</li>
<li>
<p>Scalability and resource optimization: benchmarking can be resource intensive particularly when simulating high loads or testing large models. The dynamic provisioning/de-provisioning of necessary resources with tekton can handle this well which is particularly critical for the expensive compute</p>
</li>
<li>
<p>Modular</p>
</li>
<li>
<p>Integration with existing MLOps workflows</p>
</li>
<li>
<p>Version control / auditability</p>
</li>
<li>
<p>Better handling of complex, multi-stage workflows</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>First, we&#8217;ll clone the ETX vLLM optimization repo. Then we&#8217;ll clone the benchmark pipeline repo, apply the PVC, task, and pipeline. We&#8217;ll also create an s3 bucket in Minio where the pipeline will upload the benchmark results.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Clone the ETX vLLM optimization repo.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">git clone https://github.com/redhat-ai-services/etx-llm-optimization-and-inference-leveraging.git</code></pre>
</div>
</div>
</li>
<li>
<p>Clone the GuideLLM pipeline repo.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">cd etx-llm-optimization-and-inference-leveraging
git clone https://github.com/jhurlocker/guidellm-pipeline.git</code></pre>
</div>
</div>
</li>
<li>
<p>Apply the PVC, task, and pipeline</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc apply -f guidellm-pipeline/pipeline/upload-results-task.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/guidellm-pipeline.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/pvc.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/guidellm-benchmark-task.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/mino-bucket.yaml -n vllm</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Before running the pipeline, let&#8217;s review the options for GuideLLM more closely.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_guidellm_arguments"><a class="anchor" href="#_guidellm_arguments"></a>GuideLLM Arguments</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Peruse the available GuideLLM <a href="https://github.com/neuralmagic/guidellm?tab=readme-ov-file#configurations">configuration options</a>.</p>
</li>
<li>
<p>The GitHub ReadMe gives detailed information about configuration flags</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_inputoutput_tokens"><a class="anchor" href="#_inputoutput_tokens"></a>Input/Output tokens</h3>
<div class="paragraph">
<p>For different use cases, you can set different standardized dataset profiles that can be passed in as arguments in GuideLLM. For example, the following variables represent input and output tokens, respectively, based on the given use case:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Chat (512/256)</strong></p>
</li>
<li>
<p><strong>RAG (4096/512)</strong></p>
</li>
<li>
<p><strong>Summarization (1024/256)</strong></p>
</li>
<li>
<p><strong>Code Generation (512/512)</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Using these profiles, we can map specific i/o token scenarios to real-world use cases to make these runs more explainable to how this impacts applications.</p>
</div>
</div>
<div class="sect2">
<h3 id="_rate_type"><a class="anchor" href="#_rate_type"></a>--rate-type</h3>
<div class="paragraph">
<p><strong>--rate-type</strong> defines the type of benchmark to run. By default GuideLLM will do a sweep of available benchmarks, but you may choose to isolate specific benchmark tests.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>synchronous: Runs a single stream of requests one at a time. --rate must not be set for this mode.</p>
</li>
<li>
<p>throughput: Runs all requests in parallel to measure the maximum throughput for the server (bounded by GUIDELLM__MAX_CONCURRENCY config argument). --rate must not be set for this mode.</p>
</li>
<li>
<p>concurrent: Runs a fixed number of streams of requests in parallel. --rate must be set to the desired concurrency level/number of streams.</p>
</li>
<li>
<p>constant: Sends requests asynchronously at a constant rate set by --rate.</p>
</li>
<li>
<p>poisson: Sends requests at a rate following a Poisson distribution with the mean set by --rate.</p>
</li>
<li>
<p>sweep: Automatically determines the minimum and maximum rates the server can support by running synchronous and throughput benchmarks, and then runs a series of benchmarks equally spaced between the two rates. The number of benchmarks is set by --rate (default is 10).</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_data"><a class="anchor" href="#_data"></a>--data</h3>
<div class="paragraph">
<p>GuideLLM has a default dataset it uses if you do not specify anything specific. However, the dataset you use should align with the customer use case you are working on.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_use_case_specific_data_requirements"><a class="anchor" href="#_use_case_specific_data_requirements"></a>Use-Case Specific Data Requirements</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_training_vs_production_data"><a class="anchor" href="#_training_vs_production_data"></a>Training vs Production Data</h3>
<div class="paragraph">
<p><strong>This training uses emulated data</strong> for consistency:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{"type":"emulated","prompt_tokens":512,"output_tokens":128}</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>For client engagements</strong>, use representative data for accurate performance evaluation.</p>
</div>
</div>
<div class="sect2">
<h3 id="_why_client_data_matters"><a class="anchor" href="#_why_client_data_matters"></a>Why Client Data Matters</h3>
<div class="paragraph">
<p>Real workloads differ significantly from stock data:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Token distribution</strong>: Customer support (50-200 tokens typical) vs RAG (4K+ tokens)</p>
</li>
<li>
<p><strong>Response variability</strong>: Fixed 128 tokens vs 50-800 token range in production</p>
</li>
<li>
<p><strong>Processing patterns</strong>: Math reasoning vs creative writing stress different components</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Performance Impact</strong>: Real data typically shows 25-40% higher latency variance and 2-5x difference in P99 metrics.</p>
</div>
</div>
<div class="sect2">
<h3 id="_production_evaluation_approach"><a class="anchor" href="#_production_evaluation_approach"></a>Production Evaluation Approach</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Baseline</strong>: Use stock data for initial estimates</p>
</li>
<li>
<p><strong>Validation</strong>: Test with client sample data</p>
</li>
<li>
<p><strong>Production</strong>: Use historical logs for final sizing</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Client Data Collection</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "type": "file",
  "path": "/path/to/client_sample.jsonl",
  "sample_size": 1000
}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_technical_consulting_guidelines"><a class="anchor" href="#_technical_consulting_guidelines"></a>Technical Consulting Guidelines</h3>
<div class="paragraph">
<p><strong>During Discovery</strong>:
- Request sample queries (80% typical usage)
- Identify peak patterns and edge cases</p>
</div>
<div class="paragraph">
<p><strong>During PoC</strong>:
- Start with stock data for baseline
- Compare with client data to quantify differences
- Plan 20-30% performance buffer</p>
</div>
<div class="paragraph">
<p><strong>Stock Data Limitations</strong>:
- <strong>Tests well</strong>: Infrastructure capacity, relative comparisons, scaling
- <strong>Misses</strong>: Real workload complexity, traffic variations, domain-specific patterns</p>
</div>
<div class="paragraph">
<p><strong>Key Takeaway</strong>: Stock data for learning; client data for production recommendations.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_execute_the_pipeline"><a class="anchor" href="#_execute_the_pipeline"></a>Execute the pipeline</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Set your external model inference endpoint</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">export INFERENCE_ENDPOINT=$(oc get inferenceservice granite-8b -n vllm -o jsonpath='{.status.url}')</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Make sure your granite-8b model is deployed on OpenShift AI. If you need to deploy it run:<br>
<code>helm upgrade -i granite-8b redhat-ai-services/vllm-kserve --version 0.5.11 \
  --values workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml -n vllm</code>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Run the pipeline with necessary parameters in a terminal. Accept the defaults when prompted. If you chose a different model adjust the <strong>target</strong> parameter.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">tkn pipeline start guidellm-benchmark-pipeline -n vllm \
  --param target=$INFERENCE_ENDPOINT/v1 \
  --param model-name="granite-8b" \
  --param processor="ibm-granite/granite-3.3-8b-instruct" \
  --param data-config="prompt_tokens=512,output_tokens=128" \
  --param max-seconds="30" \
  --param huggingface-token="" \
  --param api-key="" \
  --param rate="2" \
  --param rate-type="sweep" \
  --param max-concurrency="10" \
  --workspace name=shared-workspace,claimName=guidellm-output-pvc</code></pre>
</div>
</div>
<div class="paragraph">
<p>Download the benchmark results from the <strong>guidellm-benchmark</strong> bucket in Minio and open the <strong>benchmark-&lt;TIMESTAMP&gt;.txt</strong> in a text editor.</p>
</div>
<div class="paragraph">
<p>Get the route to the Minio UI. The login is <strong><em>admin/admin123</em></strong></p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc get route minio-ui -n ic-shared-minio -o jsonpath='{.spec.host}'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Minio bucket</p>
</div>
<div class="imageblock unresolved">
<div class="content">
<img src="benchmark_results_file_in_minio.png" alt="benchmark results in Minio">
</div>
</div>
<div class="paragraph">
<p>Benchmark results</p>
</div>
<div class="imageblock unresolved">
<div class="content">
<img src="benchmark_results_minio.png" alt="benchmark results">
</div>
</div>
<div class="sect2">
<h3 id="_evaluate_output_and_adjust_guidellm_settings"><a class="anchor" href="#_evaluate_output_and_adjust_guidellm_settings"></a>Evaluate Output and Adjust GuideLLM Settings</h3>
<div class="paragraph">
<p>GuideLLM captures the following metrics during a full sweep:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Requests per Second</strong>: Total requests completed per second</p>
</li>
<li>
<p><strong>Request concurrency</strong>: average concurrent requests</p>
</li>
<li>
<p><strong>Output token per second (mean)</strong>: output tokens per second</p>
</li>
<li>
<p><strong>Total tokens per second (mean)</strong>: total (prompt + output) tokens per second</p>
</li>
<li>
<p><strong>Request latency in ms (mean, median, p99)</strong>: total end to end request latency</p>
</li>
<li>
<p><strong>Time to First Token (mean, median, p99)</strong></p>
</li>
<li>
<p><strong>Inter-Token Latency (mean, median, p99)</strong></p>
</li>
<li>
<p><strong>Time per output token (mean, median, p99)</strong></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>See the complete <a href="https://github.com/neuralmagic/guidellm/blob/main/docs/metrics.md" target="_blank" rel="noopener">metrics documentation</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_reading_output"><a class="anchor" href="#_reading_output"></a>Reading Output</h3>
<div class="sect3">
<h4 id="_top_section_benchmark_info"><a class="anchor" href="#_top_section_benchmark_info"></a>Top Section (Benchmark Info)</h4>
<div class="ulist">
<ul>
<li>
<p>Benchmark: The type of benchmark ran</p>
<div class="ulist">
<ul>
<li>
<p>constant@x indicates the number of requests sent constantly to the model per second.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Requests Made: How many requests issued (completed, incomplete or errors)</p>
</li>
<li>
<p>Token Data</p>
<div class="ulist">
<ul>
<li>
<p>Tok/Req: average tokens per request</p>
</li>
<li>
<p>Tok Total: total number of input/output tokens processed</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_bottom_section_benchmark_stats"><a class="anchor" href="#_bottom_section_benchmark_stats"></a>Bottom Section (Benchmark Stats)</h4>
<div class="ulist">
<ul>
<li>
<p>Mean</p>
<div class="ulist">
<ul>
<li>
<p>Overall average</p>
</li>
<li>
<p>Good for general performance overview</p>
</li>
</ul>
</div>
</li>
<li>
<p>Median</p>
<div class="ulist">
<ul>
<li>
<p>Typical experience</p>
</li>
<li>
<p>More stable, less skewed by outliers</p>
</li>
</ul>
</div>
</li>
<li>
<p>P99</p>
<div class="ulist">
<ul>
<li>
<p>Worst-case real latency</p>
</li>
<li>
<p>Essential for SLOs and user experience under load</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_adjusting_guidellm_settings"><a class="anchor" href="#_adjusting_guidellm_settings"></a>Adjusting GuideLLM Settings</h3>
<div class="paragraph">
<p>Depending on the results, try running GuideLLM a couple of different ways to see how the different controlled tests impact results.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_advanced_performance_evaluation_exercises"><a class="anchor" href="#_advanced_performance_evaluation_exercises"></a>Advanced Performance Evaluation Exercises</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For advanced engagements, it&#8217;s crucial to demonstrate how different workload characteristics impact performance. The following exercises provide specific scenarios that align with common client use cases.</p>
</div>
<div class="sect2">
<h3 id="_exercise_1_token_size_impact_analysis"><a class="anchor" href="#_exercise_1_token_size_impact_analysis"></a>Exercise 1: Token Size Impact Analysis</h3>
<div class="paragraph">
<p>Understanding how input/output token ratios affect performance is essential for capacity planning and cost estimation.</p>
</div>
<div class="sect3">
<h4 id="_exercise_1a_chat_application_simulation"><a class="anchor" href="#_exercise_1a_chat_application_simulation"></a>Exercise 1a: Chat Application Simulation</h4>
<div class="paragraph">
<p>Test a typical conversational AI scenario with short prompts and responses:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">tkn pipeline start guidellm-benchmark-pipeline -n vllm \
  --param target=$INFERENCE_ENDPOINT/v1 \
  --param model-name="granite-8b" \
  --param processor="ibm-granite/granite-3.3-8b-instruct" \
  --param data-config="prompt_tokens=256,output_tokens=128" \
  --param max-seconds="30" \
  --param huggingface-token="" \
  --param api-key="" \
  --param rate="2" \
  --param rate-type="sweep" \
  --param max-concurrency="10" \
  --workspace name=shared-workspace,claimName=guidellm-output-pvc</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Business Context</strong>: Represents customer service chatbots, virtual assistants, or interactive coding assistants where users expect rapid, conversational responses.</p>
</div>
</div>
<div class="sect3">
<h4 id="_exercise_1b_rag_retrieval_augmented_generation_simulation"><a class="anchor" href="#_exercise_1b_rag_retrieval_augmented_generation_simulation"></a>Exercise 1b: RAG (Retrieval-Augmented Generation) Simulation</h4>
<div class="paragraph">
<p>Test document-heavy workloads with large context windows:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">tkn pipeline start guidellm-benchmark-pipeline -n vllm \
  --param target=$INFERENCE_ENDPOINT/v1 \
  --param model-name="granite-8b" \
  --param processor="ibm-granite/granite-3.3-8b-instruct" \
  --param data-config="prompt_tokens=4096,output_tokens=512" \
  --param max-seconds="30" \
  --param huggingface-token="" \
  --param api-key="" \
  --param rate="2" \
  --param rate-type="sweep" \
  --param max-concurrency="10" \
  --workspace name=shared-workspace,claimName=guidellm-output-pvc</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Business Context</strong>: Enterprise knowledge base queries, document analysis, or research assistance where large amounts of context are processed.</p>
</div>
</div>
<div class="sect3">
<h4 id="_exercise_1c_code_generation_workload"><a class="anchor" href="#_exercise_1c_code_generation_workload"></a>Exercise 1c: Code Generation Workload</h4>
<div class="paragraph">
<p>Test balanced input/output for development use cases:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">tkn pipeline start guidellm-benchmark-pipeline -n vllm \
  --param target=$INFERENCE_ENDPOINT/v1 \
  --param model-name="granite-8b" \
  --param processor="ibm-granite/granite-3.3-8b-instruct" \
  --param data-config="prompt_tokens=512,output_tokens=512" \
  --param max-seconds="30" \
  --param huggingface-token="" \
  --param api-key="" \
  --param rate="2" \
  --param rate-type="sweep" \
  --param max-concurrency="10" \
  --workspace name=shared-workspace,claimName=guidellm-output-pvc</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Business Context</strong>: AI-powered development tools, code completion, and automated programming assistance.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_exercise_2_rate_type_deep_dive"><a class="anchor" href="#_exercise_2_rate_type_deep_dive"></a>Exercise 2: Rate Type Deep Dive</h3>
<div class="paragraph">
<p>Different rate types reveal distinct performance characteristics critical for technical consulting. Select one option to test during this exercise due to time restrictions.</p>
</div>
<div class="sect3">
<h4 id="_exercise_2a_peak_capacity_assessment_throughput"><a class="anchor" href="#_exercise_2a_peak_capacity_assessment_throughput"></a>Exercise 2a: Peak Capacity Assessment (Throughput)</h4>
<div class="paragraph">
<p>Determine maximum theoretical performance:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">tkn pipeline start guidellm-benchmark-pipeline -n vllm \
  --param target=$INFERENCE_ENDPOINT/v1 \
  --param model-name="granite-8b" \
  --param processor="ibm-granite/granite-3.3-8b-instruct" \
  --param data-config="prompt_tokens=512,output_tokens=256" \
  --param max-seconds="30" \
  --param huggingface-token="" \
  --param api-key="" \
  --param rate="2" \
  --param rate-type="throughput" \
  --param max-concurrency="10" \
  --workspace name=shared-workspace,claimName=guidellm-output-pvc</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Technical Consulting Value</strong>:
- Establishes theoretical maximum capacity for infrastructure sizing
- Identifies hardware bottlenecks and scaling limits
- Provides baseline for capacity planning and cost modeling</p>
</div>
</div>
<div class="sect3">
<h4 id="_exercise_2b_real_world_load_simulation_constant"><a class="anchor" href="#_exercise_2b_real_world_load_simulation_constant"></a>Exercise 2b: Real-World Load Simulation (Constant)</h4>
<div class="paragraph">
<p>Test sustained production loads:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">tkn pipeline start guidellm-benchmark-pipeline -n vllm \
  --param target=$INFERENCE_ENDPOINT/v1 \
  --param model-name="granite-8b" \
  --param processor="ibm-granite/granite-3.3-8b-instruct" \
  --param data-config="prompt_tokens=512,output_tokens=256" \
  --param max-seconds="30" \
  --param huggingface-token="" \
  --param api-key="" \
  --param rate="2" \
  --param rate-type="constant" \
  --param max-concurrency="10" \
  --workspace name=shared-workspace,claimName=guidellm-output-pvc</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Technical Consulting Value</strong>:
- Validates performance under realistic sustained loads
- Identifies latency degradation patterns as load increases
- Supports SLA definition and performance guarantees</p>
</div>
</div>
<div class="sect3">
<h4 id="_exercise_2c_burst_traffic_analysis_poisson"><a class="anchor" href="#_exercise_2c_burst_traffic_analysis_poisson"></a>Exercise 2c: Burst Traffic Analysis (Poisson)</h4>
<div class="paragraph">
<p>Test irregular, bursty workloads typical in enterprise environments:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">tkn pipeline start guidellm-benchmark-pipeline -n vllm \
  --param target=$INFERENCE_ENDPOINT/v1 \
  --param model-name="granite-8b" \
  --param processor="ibm-granite/granite-3.3-8b-instruct" \
  --param data-config="prompt_tokens=512,output_tokens=256" \
  --param max-seconds="30" \
  --param huggingface-token="" \
  --param api-key="" \
  --param rate="2" \
  --param rate-type="poisson" \
  --param max-concurrency="10" \
  --workspace name=shared-workspace,claimName=guidellm-output-pvc</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Technical Consulting Value</strong>:
- Models real-world traffic patterns with natural variability
- Reveals queue management and batching effectiveness
- Supports autoscaling configuration and resource allocation</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_exercise_3_comparative_analysis_framework"><a class="anchor" href="#_exercise_3_comparative_analysis_framework"></a>Exercise 3: Comparative Analysis Framework</h3>
<div class="paragraph">
<p>Run multiple configurations to build performance profiles for client decision-making:</p>
</div>
<div class="sect3">
<h4 id="_token_scaling_analysis"><a class="anchor" href="#_token_scaling_analysis"></a>Token Scaling Analysis</h4>
<div class="paragraph">
<p>Execute all three token configurations sequentially and compare:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Baseline</strong> (Chat): 256/128 tokens</p>
</li>
<li>
<p><strong>Medium</strong> (Mixed): 1024/256 tokens</p>
</li>
<li>
<p><strong>Heavy</strong> (RAG): 4096/512 tokens</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Analysis Points for Technical Consulting</strong>:
- <strong>Memory Usage Scaling</strong>: How does KV cache grow with context length?
- <strong>Latency Patterns</strong>: Linear vs exponential increases with token count
- <strong>Throughput Impact</strong>: Requests/second degradation with larger contexts
- <strong>Cost Implications</strong>: GPU hours required for different workload types</p>
</div>
</div>
<div class="sect3">
<h4 id="_rate_type_performance_matrix"><a class="anchor" href="#_rate_type_performance_matrix"></a>Rate Type Performance Matrix</h4>
<div class="paragraph">
<p>Test each rate type with consistent token configuration to isolate performance characteristics:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Synchronous</strong>: Baseline single-request latency</p>
</li>
<li>
<p><strong>Constant</strong>: Sustained load performance</p>
</li>
<li>
<p><strong>Poisson</strong>: Variable load handling</p>
</li>
<li>
<p><strong>Sweep</strong>: Comprehensive performance curve</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Technical Consulting Applications</strong>:
- <strong>Infrastructure Sizing</strong>: Use throughput results for hardware recommendations
- <strong>SLA Development</strong>: Leverage latency percentiles for performance guarantees
- <strong>Cost Modeling</strong>: Apply sustained load results to pricing calculations
- <strong>Scaling Strategy</strong>: Use sweep results to plan horizontal scaling triggers</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_enhanced_metrics_interpretation"><a class="anchor" href="#_enhanced_metrics_interpretation"></a>Enhanced Metrics Interpretation</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_critical_performance_indicators"><a class="anchor" href="#_critical_performance_indicators"></a>Critical Performance Indicators</h3>
<div class="sect3">
<h4 id="_time_to_first_token_ttft"><a class="anchor" href="#_time_to_first_token_ttft"></a>Time to First Token (TTFT)</h4>
<div class="paragraph">
<p><strong>Business Impact</strong>: Direct correlation to user experience and perceived responsiveness
- <strong>Target</strong>: &lt;200ms for interactive applications
- <strong>Acceptable</strong>: 200-500ms for productivity tools
- <strong>Problematic</strong>: &gt;500ms indicates infrastructure or model optimization issues</p>
</div>
<div class="paragraph">
<p><strong>Technical Consulting Guidance</strong>:
- High TTFT often indicates memory bandwidth limitations
- Consistent across rate types suggests model-level bottlenecks
- Variable TTFT indicates queueing or resource contention</p>
</div>
</div>
<div class="sect3">
<h4 id="_inter_token_latency_itl"><a class="anchor" href="#_inter_token_latency_itl"></a>Inter-Token Latency (ITL)</h4>
<div class="paragraph">
<p><strong>Business Impact</strong>: Affects streaming response quality and user engagement
- <strong>Target</strong>: &lt;50ms for smooth streaming experience
- <strong>Monitoring</strong>: P99 values reveal worst-case user experience
- <strong>Optimization</strong>: Focus on batching efficiency and memory management</p>
</div>
</div>
<div class="sect3">
<h4 id="_request_latency_distribution_analysis"><a class="anchor" href="#_request_latency_distribution_analysis"></a>Request Latency Distribution Analysis</h4>
<div class="paragraph">
<p><strong>For Technical Consulting</strong>:
- <strong>Mean</strong>: General performance overview, useful for capacity planning
- <strong>Median</strong>: Typical user experience, critical for SLA commitments
- <strong>P99</strong>: Tail latency, essential for user satisfaction and system reliability</p>
</div>
<div class="paragraph">
<p><strong>Red Flags</strong>:
- Large gap between median and P99 indicates inconsistent performance
- Degrading P99 under load suggests approaching capacity limits
- High variability points to resource contention or inefficient scheduling</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_business_alignment_framework"><a class="anchor" href="#_business_alignment_framework"></a>Business Alignment Framework</h3>
<div class="sect3">
<h4 id="_cost_performance_analysis"><a class="anchor" href="#_cost_performance_analysis"></a>Cost-Performance Analysis</h4>
<div class="paragraph">
<p>Map performance metrics to business value:</p>
</div>
<div class="paragraph">
<p><strong>Throughput-Based Costing</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">Cost per Request = (GPU Hours x Hourly Rate) / Total Requests Processed</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Quality-of-Service Tiers</strong>:
- <strong>Premium</strong>: P99 &lt; 500ms, High throughput, Premium pricing
- <strong>Standard</strong>: P99 &lt; 1000ms, Medium throughput, Standard pricing
- <strong>Economy</strong>: P99 &lt; 2000ms, Lower throughput, Budget pricing</p>
</div>
</div>
<div class="sect3">
<h4 id="_capacity_planning_recommendations"><a class="anchor" href="#_capacity_planning_recommendations"></a>Capacity Planning Recommendations</h4>
<div class="paragraph">
<p><strong>Based on Sweep Results</strong>:
1. <strong>Peak Efficiency Point</strong>: Identify request rate with optimal cost/performance ratio
2. <strong>Linear Scaling Range</strong>: Determine where performance degrades linearly vs exponentially
3. <strong>Breaking Point</strong>: Establish maximum sustainable load before quality degradation</p>
</div>
<div class="paragraph">
<p><strong>Infrastructure Sizing Formula</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">Required GPUs = (Peak Expected RPS x Safety Margin) / Sustainable RPS per GPU</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_troubleshooting_performance_issues"><a class="anchor" href="#_troubleshooting_performance_issues"></a>Troubleshooting Performance Issues</h3>
<div class="sect3">
<h4 id="_high_latency_diagnosis"><a class="anchor" href="#_high_latency_diagnosis"></a>High Latency Diagnosis</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>TTFT &gt; ITL</strong>: Memory bandwidth or model loading bottleneck</p>
</li>
<li>
<p><strong>ITL &gt;&gt; TTFT</strong>: Compute or batching inefficiency</p>
</li>
<li>
<p><strong>Both High</strong>: Infrastructure under-sizing or configuration issues</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_low_throughput_diagnosis"><a class="anchor" href="#_low_throughput_diagnosis"></a>Low Throughput Diagnosis</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Compare synchronous vs throughput</strong>: Reveals batching effectiveness</p>
</li>
<li>
<p><strong>Monitor GPU utilization</strong>: Low utilization indicates non-GPU bottlenecks</p>
</li>
<li>
<p><strong>Analyze queue depths</strong>: High queuing suggests insufficient parallelism</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_inconsistent_performance_diagnosis"><a class="anchor" href="#_inconsistent_performance_diagnosis"></a>Inconsistent Performance Diagnosis</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>P99 &gt;&gt; Median</strong>: Resource contention or thermal throttling</p>
</li>
<li>
<p><strong>Variable between runs</strong>: External factors or inadequate warm-up</p>
</li>
<li>
<p><strong>Degradation over time</strong>: Memory leaks or resource exhaustion</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This comprehensive evaluation framework enables technical consultants to provide data-driven recommendations for LLM deployment optimization, infrastructure sizing, and cost management.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary"><a class="anchor" href="#_summary"></a>Summary</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This activity demonstrated how to evaluate system performance using GuideLLM with a default vLLM configuration. By configuring vLLM more precisely or your chosen inference runtime, you can better align model serving with application needs—whether you’re optimizing for cost, speed, or user experience.
nding directory for the chapter accordingly.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="../LABENV/index.html">Lab Environment</a></span>
  <span class="next"><a href="mission.html">Your Place in the Adventure</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
