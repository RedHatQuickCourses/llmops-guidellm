<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Course: Model Performance Benchmarking with GuideLLM :: Model Performance Benchmarking with GuideLLM</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="section2.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Model Performance Benchmarking with GuideLLM</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-guidellm" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Model Performance Benchmarking with GuideLLM</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">:imagesdir: ../assets/images</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="section1.html">Course: Model Performance Benchmarking with GuideLLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section2.html">Section 2</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">Section 3</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Chapter 2</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Section 1</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Chapter 3</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section1.html">Section 1</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section2.html">Section 2</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Model Performance Benchmarking with GuideLLM</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Model Performance Benchmarking with GuideLLM</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Model Performance Benchmarking with GuideLLM</a></li>
    <li><a href="index.html">:imagesdir: ../assets/images</a></li>
    <li><a href="section1.html">Course: Model Performance Benchmarking with GuideLLM</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Course: Model Performance Benchmarking with GuideLLM</h1>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_introduction_to_performance_benchmarking">1. Introduction to Performance Benchmarking</a>
<ul class="sectlevel2">
<li><a href="#_why_performance_evaluation_matters">1.1. Why Performance Evaluation Matters</a></li>
<li><a href="#_what_is_guidellm">1.2. What is GuideLLM?</a></li>
</ul>
</li>
<li><a href="#_module_1_setting_up_your_benchmarking_environment">2. Module 1: Setting Up Your Benchmarking Environment</a>
<ul class="sectlevel2">
<li><a href="#_1_1_install_the_tekton_cli">2.1. 1.1 Install the Tekton CLI</a></li>
<li><a href="#_1_2_deploy_the_guidellm_pipeline_resources">2.2. 1.2 Deploy the GuideLLM Pipeline Resources</a></li>
</ul>
</li>
<li><a href="#_module_2_running_your_first_benchmark">3. Module 2: Running Your First Benchmark</a>
<ul class="sectlevel2">
<li><a href="#_2_1_prepare_the_target_model">3.1. 2.1 Prepare the Target Model</a></li>
<li><a href="#_2_2_execute_the_pipeline">3.2. 2.2 Execute the Pipeline</a></li>
<li><a href="#_2_3_retrieve_and_view_results">3.3. 2.3 Retrieve and View Results</a></li>
</ul>
</li>
<li><a href="#_module_3_interpreting_benchmark_results">4. Module 3: Interpreting Benchmark Results</a>
<ul class="sectlevel2">
<li><a href="#_3_1_key_guidellm_metrics">4.1. 3.1 Key GuideLLM Metrics</a></li>
<li><a href="#_3_2_reading_the_output_file">4.2. 3.2 Reading the Output File</a>
<ul class="sectlevel3">
<li><a href="#_top_section_benchmark_info">4.2.1. Top Section (Benchmark Info)</a></li>
<li><a href="#_bottom_section_benchmark_stats">4.2.2. Bottom Section (Benchmark Stats)</a></li>
</ul>
</li>
<li><a href="#_3_3_the_importance_of_data_in_benchmarking">4.3. 3.3 The Importance of Data in Benchmarking</a></li>
</ul>
</li>
<li><a href="#_module_4_advanced_benchmarking_scenarios">5. Module 4: Advanced Benchmarking Scenarios</a>
<ul class="sectlevel2">
<li><a href="#_4_1_exercise_1_token_size_impact_analysis">5.1. 4.1 Exercise 1: Token Size Impact Analysis</a>
<ul class="sectlevel3">
<li><a href="#_4_1a_chat_application_simulation_short_io">5.1.1. 4.1a: Chat Application Simulation (Short I/O)</a></li>
<li><a href="#_4_1b_rag_simulation_large_input">5.1.2. 4.1b: RAG Simulation (Large Input)</a></li>
<li><a href="#_4_1c_code_generation_workload_balanced_io">5.1.3. 4.1c: Code Generation Workload (Balanced I/O)</a></li>
</ul>
</li>
<li><a href="#_4_2_exercise_2_rate_type_deep_dive">5.2. 4.2 Exercise 2: Rate Type Deep Dive</a>
<ul class="sectlevel3">
<li><a href="#_4_2a_peak_capacity_assessment_throughput">5.2.1. 4.2a: Peak Capacity Assessment (<code>throughput</code>)</a></li>
<li><a href="#_4_2b_real_world_load_simulation_constant">5.2.2. 4.2b: Real-World Load Simulation (<code>constant</code>)</a></li>
<li><a href="#_4_2c_burst_traffic_analysis_poisson">5.2.3. 4.2c: Burst Traffic Analysis (<code>poisson</code>)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_course_wrap_up">6. Course Wrap-up</a>
<ul class="sectlevel2">
<li><a href="#_summary_of_learnings">6.1. Summary of Learnings</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="_introduction_to_performance_benchmarking"><a class="anchor" href="#_introduction_to_performance_benchmarking"></a>1. Introduction to Performance Benchmarking</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to the Model Performance Benchmarking with GuideLLM course. In this course, you will learn how to quantitatively measure, analyze, and optimize the performance of Large Language Models deployed on Red Hat OpenShift AI.</p>
</div>
<div class="paragraph">
<p>By the end of this course, you will be able to:
* Set up an automated benchmarking pipeline using GuideLLM and Tekton.
* Execute various performance tests that simulate real-world workloads.
* Interpret key performance metrics like latency, throughput, and token generation speed.
* Connect performance results to business outcomes, such as user experience and infrastructure cost.</p>
</div>
<div class="sect2">
<h3 id="_why_performance_evaluation_matters"><a class="anchor" href="#_why_performance_evaluation_matters"></a>1.1. Why Performance Evaluation Matters</h3>
<div class="paragraph">
<p>In Generative AI systems, evaluating system performance including latency, throughput, and resource utilization is just as important as evaluating model accuracy or quality. Here&#8217;s why:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>User Experience</strong>: High latency leads to sluggish interactions, which is unacceptable in chatbots, copilots, and real-time applications. Users expect sub-second responses.</p>
</li>
<li>
<p><strong>Scalability</strong>: Throughput determines how many requests a system can handle in parallel. For enterprise GenAI apps, high throughput is essential to serve multiple users or integrate with high-frequency backend processes.</p>
</li>
<li>
<p><strong>Cost Efficiency</strong>: Slow or inefficient systems require more compute to serve the same volume of requests. Optimizing performance directly reduces cloud GPU costs and improves ROI.</p>
</li>
<li>
<p><strong>Fair Benchmarking</strong>: A model may appear “better” in isolated evaluation, but if it requires excessive inference time or hardware, it may not be viable in production. True model evaluation must balance quality and performance.</p>
</li>
<li>
<p><strong>Deployment Readiness</strong>: Latency and throughput impact architectural decisions (e.g., batching, caching, distributed serving). Measuring them ensures a model is viable under real-world constraints.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_what_is_guidellm"><a class="anchor" href="#_what_is_guidellm"></a>1.2. What is GuideLLM?</h3>
<div class="paragraph">
<p><strong>GuideLLM</strong> is a toolkit for evaluating and optimizing the deployment of LLMs. By simulating real-world inference workloads, GuideLLM enables you to easily assess the performance, resource requirements, and cost implications of deploying LLMs on various hardware configurations. This approach ensures efficient, scalable, and cost-effective LLM inference serving while maintaining high service quality.</p>
</div>
<div class="paragraph">
<p>GuideLLM is now officially a part of the vLLM upstream project. This toolset is one of the primary ways Red Hat internal teams are benchmarking customer models and will be the main framework we will recommend to our customers.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
It&#8217;s important to distinguish GuideLLM from Trusty AI. Trusty AI&#8217;s scope is responsible AI (explainability, fairness, bias detection), while GuideLLM is focused purely on system performance benchmarking and optimization.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_1_setting_up_your_benchmarking_environment"><a class="anchor" href="#_module_1_setting_up_your_benchmarking_environment"></a>2. Module 1: Setting Up Your Benchmarking Environment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this module, we will set up an automated Tekton pipeline on OpenShift AI to run our GuideLLM benchmarks. Using a Tekton pipeline provides automation, reproducibility, and efficient resource management for our tests.</p>
</div>
<div class="sect2">
<h3 id="_1_1_install_the_tekton_cli"><a class="anchor" href="#_1_1_install_the_tekton_cli"></a>2.1. 1.1 Install the Tekton CLI</h3>
<div class="paragraph">
<p>First, ensure the Tekton CLI (<code>tkn</code>) is installed in your terminal. This tool will allow us to interact with our pipelines.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">curl -sL $(curl -s https://api.github.com/repos/tektoncd/cli/releases/latest | grep "browser_download_url.*_Linux_x86_64.tar.gz" | cut -d '"' -f 4) | sudo tar -xz -C /usr/local/bin tkn
tkn version</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_1_2_deploy_the_guidellm_pipeline_resources"><a class="anchor" href="#_1_2_deploy_the_guidellm_pipeline_resources"></a>2.2. 1.2 Deploy the GuideLLM Pipeline Resources</h3>
<div class="paragraph">
<p>Next, we will clone the necessary Git repositories and apply the Kubernetes resources that define our pipeline, its tasks, and the required storage.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Clone the ETX vLLM optimization repo which contains the pipeline definition.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">git clone https://github.com/redhat-ai-services/etx-llm-optimization-and-inference-leveraging.git
cd etx-llm-optimization-and-inference-leveraging</code></pre>
</div>
</div>
</li>
<li>
<p>Clone the GuideLLM pipeline repo itself.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">git clone https://github.com/jhurlocker/guidellm-pipeline.git</code></pre>
</div>
</div>
</li>
<li>
<p>Apply the PVC, task, pipeline, and Minio bucket configuration to your cluster. Ensure you are in the correct namespace (<code>vllm</code>).</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc apply -f guidellm-pipeline/pipeline/upload-results-task.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/guidellm-pipeline.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/pvc.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/guidellm-benchmark-task.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/mino-bucket.yaml -n vllm</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>With these resources created, we are now ready to execute a benchmark.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_2_running_your_first_benchmark"><a class="anchor" href="#_module_2_running_your_first_benchmark"></a>3. Module 2: Running Your First Benchmark</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now that the pipeline is set up, let&#8217;s run our first test. This will help us establish a baseline and understand the end-to-end workflow.</p>
</div>
<div class="sect2">
<h3 id="_2_1_prepare_the_target_model"><a class="anchor" href="#_2_1_prepare_the_target_model"></a>3.1. 2.1 Prepare the Target Model</h3>
<div class="paragraph">
<p>Before running the pipeline, ensure your target model is deployed and accessible. For this lab, we will use the <code>granite-8b</code> model.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Get the inference endpoint URL for your deployed model.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">export INFERENCE_ENDPOINT=$(oc get inferenceservice granite-8b -n vllm -o jsonpath='{.status.url}')
echo $INFERENCE_ENDPOINT</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you have not deployed the <code>granite-8b</code> model yet, run the following Helm command: <code>helm upgrade -i granite-8b redhat-ai-services/vllm-kserve --version 0.5.11 --values workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml -n vllm</code>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_2_2_execute_the_pipeline"><a class="anchor" href="#_2_2_execute_the_pipeline"></a>3.2. 2.2 Execute the Pipeline</h3>
<div class="paragraph">
<p>Now, we will start the Tekton pipeline. We&#8217;ll pass parameters to define the target model, the workload profile (<code>data-config</code>), and the test type (<code>rate-type</code>). We will use a <code>sweep</code> which automatically tests a range of request rates.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">tkn pipeline start guidellm-benchmark-pipeline -n vllm \
  --param target=$INFERENCE_ENDPOINT/v1 \
  --param model-name="granite-8b" \
  --param processor="ibm-granite/granite-3.3-8b-instruct" \
  --param data-config="prompt_tokens=512,output_tokens=128" \
  --param max-seconds="30" \
  --param huggingface-token="" \
  --param api-key="" \
  --param rate="2" \
  --param rate-type="sweep" \
  --param max-concurrency="10" \
  --workspace name=shared-workspace,claimName=guidellm-output-pvc</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_2_3_retrieve_and_view_results"><a class="anchor" href="#_2_3_retrieve_and_view_results"></a>3.3. 2.3 Retrieve and View Results</h3>
<div class="paragraph">
<p>The pipeline will run the benchmark and upload the results as a text file to a Minio S3 bucket.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Get the route to the Minio UI. The login is <code>admin/admin123</code>.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc get route minio-ui -n ic-shared-minio -o jsonpath='{.spec.host}'</code></pre>
</div>
</div>
</li>
<li>
<p>Navigate to the Minio URL in your browser, log in, and find the <code>guidellm-benchmark</code> bucket.</p>
<div class="imageblock unresolved">
<div class="content">
<img src="benchmark_results_file_in_minio.png" alt="benchmark results in Minio">
</div>
</div>
</li>
<li>
<p>Download the <code>benchmark-&lt;TIMESTAMP&gt;.txt</code> file and open it in a text editor to view the raw results.</p>
<div class="imageblock unresolved">
<div class="content">
<img src="benchmark_results_minio.png" alt="benchmark results">
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_3_interpreting_benchmark_results"><a class="anchor" href="#_module_3_interpreting_benchmark_results"></a>4. Module 3: Interpreting Benchmark Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Understanding the output is crucial for drawing meaningful conclusions. This module breaks down the key metrics and how to interpret them.</p>
</div>
<div class="sect2">
<h3 id="_3_1_key_guidellm_metrics"><a class="anchor" href="#_3_1_key_guidellm_metrics"></a>4.1. 3.1 Key GuideLLM Metrics</h3>
<div class="paragraph">
<p>GuideLLM captures several critical metrics. Here are the most important ones:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Requests per Second</strong>: Total requests completed per second (Throughput).</p>
</li>
<li>
<p><strong>Request concurrency</strong>: The average number of requests being processed in parallel.</p>
</li>
<li>
<p><strong>Output token per second (mean)</strong>: The generation speed of the model.</p>
</li>
<li>
<p><strong>Time to First Token (TTFT)</strong>: How quickly the first token is returned. This is critical for user-perceived responsiveness. (Measured in ms: mean, median, p99).</p>
</li>
<li>
<p><strong>Inter-Token Latency (ITL)</strong>: The time between subsequent tokens in a response. This affects the smoothness of streaming. (Measured in ms: mean, median, p99).</p>
</li>
<li>
<p><strong>Request latency</strong>: Total end-to-end time for a request. (Measured in ms: mean, median, p99).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>See the complete <a href="https://github.com/neuralmagic/guidellm/blob/main/docs/metrics.md" target="_blank" rel="noopener">metrics documentation</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_3_2_reading_the_output_file"><a class="anchor" href="#_3_2_reading_the_output_file"></a>4.2. 3.2 Reading the Output File</h3>
<div class="paragraph">
<p>The output file has two main sections.</p>
</div>
<div class="sect3">
<h4 id="_top_section_benchmark_info"><a class="anchor" href="#_top_section_benchmark_info"></a>4.2.1. Top Section (Benchmark Info)</h4>
<div class="paragraph">
<p>This section summarizes the test parameters.
* <strong>Benchmark</strong>: The type of benchmark that ran (e.g., <code>constant@2</code> indicates 2 requests per second).
* <strong>Requests Made</strong>: Total number of requests issued.
* <strong>Token Data</strong>: Average and total tokens processed.</p>
</div>
</div>
<div class="sect3">
<h4 id="_bottom_section_benchmark_stats"><a class="anchor" href="#_bottom_section_benchmark_stats"></a>4.2.2. Bottom Section (Benchmark Stats)</h4>
<div class="paragraph">
<p>This section contains the performance statistics. When analyzing these, pay close attention to the distribution:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Mean</strong>: The overall average. Good for a general performance overview.</p>
</li>
<li>
<p><strong>Median (p50)</strong>: The typical user experience. It is less skewed by a few very slow requests.</p>
</li>
<li>
<p><strong>p99</strong>: The 99th percentile, representing the worst-case latency for the vast majority of users. This is essential for defining and meeting Service Level Objectives (SLOs). A large gap between the median and p99 indicates inconsistent performance under load.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_3_3_the_importance_of_data_in_benchmarking"><a class="anchor" href="#_3_3_the_importance_of_data_in_benchmarking"></a>4.3. 3.3 The Importance of Data in Benchmarking</h3>
<div class="paragraph">
<p>The data you use for testing dramatically impacts the results.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>This training uses emulated data</strong> (<code>{"type":"emulated","prompt_tokens":512,"output_tokens":128}</code>) for consistency and simplicity.</p>
</li>
<li>
<p><strong>For client engagements, always use representative production data.</strong> Real workloads differ significantly from synthetic data in token distribution and response variability. Using client data can reveal 25-40% higher latency variance.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Key Takeaway</strong>: Use stock data for learning and initial baselines; use client data for production recommendations.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_4_advanced_benchmarking_scenarios"><a class="anchor" href="#_module_4_advanced_benchmarking_scenarios"></a>5. Module 4: Advanced Benchmarking Scenarios</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this module, you will run a series of targeted tests to simulate common customer use cases and understand how different workloads impact performance.</p>
</div>
<div class="sect2">
<h3 id="_4_1_exercise_1_token_size_impact_analysis"><a class="anchor" href="#_4_1_exercise_1_token_size_impact_analysis"></a>5.1. 4.1 Exercise 1: Token Size Impact Analysis</h3>
<div class="paragraph">
<p>Here, we&#8217;ll test how different input/output token sizes, representing different applications, affect performance.</p>
</div>
<div class="sect3">
<h4 id="_4_1a_chat_application_simulation_short_io"><a class="anchor" href="#_4_1a_chat_application_simulation_short_io"></a>5.1.1. 4.1a: Chat Application Simulation (Short I/O)</h4>
<div class="paragraph">
<p>Represents a typical conversational AI scenario.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console"># data-config="prompt_tokens=256,output_tokens=128"
# (Execute the tkn pipeline start command with the above data-config)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_4_1b_rag_simulation_large_input"><a class="anchor" href="#_4_1b_rag_simulation_large_input"></a>5.1.2. 4.1b: RAG Simulation (Large Input)</h4>
<div class="paragraph">
<p>Represents enterprise knowledge base queries with large context.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console"># data-config="prompt_tokens=4096,output_tokens=512"
# (Execute the tkn pipeline start command with the above data-config)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_4_1c_code_generation_workload_balanced_io"><a class="anchor" href="#_4_1c_code_generation_workload_balanced_io"></a>5.1.3. 4.1c: Code Generation Workload (Balanced I/O)</h4>
<div class="paragraph">
<p>Represents AI-powered developer tools.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console"># data-config="prompt_tokens=512,output_tokens=512"
# (Execute the tkn pipeline start command with the above data-config)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Analysis</strong>: Compare the TTFT, output tokens/sec, and overall latency across these three runs. You will likely see that larger token counts significantly increase latency and reduce throughput.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_4_2_exercise_2_rate_type_deep_dive"><a class="anchor" href="#_4_2_exercise_2_rate_type_deep_dive"></a>5.2. 4.2 Exercise 2: Rate Type Deep Dive</h3>
<div class="paragraph">
<p>Different <code>--rate-type</code> arguments reveal distinct performance characteristics.</p>
</div>
<div class="sect3">
<h4 id="_4_2a_peak_capacity_assessment_throughput"><a class="anchor" href="#_4_2a_peak_capacity_assessment_throughput"></a>5.2.1. 4.2a: Peak Capacity Assessment (<code>throughput</code>)</h4>
<div class="paragraph">
<p>This test sends all requests in parallel to find the theoretical maximum capacity of the server.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console"># rate-type="throughput"
# (Execute the tkn pipeline start command with the above rate-type)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Consulting Value</strong>: Establishes the upper bound for infrastructure sizing and identifies hardware bottlenecks.</p>
</div>
</div>
<div class="sect3">
<h4 id="_4_2b_real_world_load_simulation_constant"><a class="anchor" href="#_4_2b_real_world_load_simulation_constant"></a>5.2.2. 4.2b: Real-World Load Simulation (<code>constant</code>)</h4>
<div class="paragraph">
<p>This test sends requests at a steady, sustained rate.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console"># rate-type="constant"
# (Execute the tkn pipeline start command with the above rate-type and a specific --rate, e.g., --rate="4")</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Consulting Value</strong>: Validates performance under realistic production loads and helps define SLAs.</p>
</div>
</div>
<div class="sect3">
<h4 id="_4_2c_burst_traffic_analysis_poisson"><a class="anchor" href="#_4_2c_burst_traffic_analysis_poisson"></a>5.2.3. 4.2c: Burst Traffic Analysis (<code>poisson</code>)</h4>
<div class="paragraph">
<p>This test sends requests at an irregular, bursty rate, which models many real-world traffic patterns.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console"># rate-type="poisson"
# (Execute the tkn pipeline start command with the above rate-type and a mean --rate)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Consulting Value</strong>: Reveals how well the system handles unpredictable traffic spikes and informs autoscaling configurations.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_course_wrap_up"><a class="anchor" href="#_course_wrap_up"></a>6. Course Wrap-up</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Congratulations on completing the Model Performance Benchmarking course.</p>
</div>
<div class="sect2">
<h3 id="_summary_of_learnings"><a class="anchor" href="#_summary_of_learnings"></a>6.1. Summary of Learnings</h3>
<div class="paragraph">
<p>In this course, we demonstrated how to evaluate system performance for a deployed Large Language Model using GuideLLM.</p>
</div>
<div class="paragraph">
<p>You learned to:
* Set up an automated testing environment using a <strong>Tekton pipeline</strong>.
* Run benchmarks using different <strong>workload profiles</strong> (token sizes) and <strong>rate types</strong> (throughput, constant, sweep) to simulate real-world scenarios.
* Retrieve and analyze key performance metrics, focusing on <strong>latency (TTFT, ITL)</strong> and <strong>throughput</strong>.
* Understand the critical difference between the <strong>mean, median, and p99</strong> percentiles for assessing user experience and defining SLOs.</p>
</div>
<div class="paragraph">
<p>By mastering these skills, you can better align model serving with application needs—whether you’re optimizing for cost, speed, or user experience—and provide data-driven recommendations that deliver significant value to customers.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">:imagesdir: ../assets/images</a></span>
  <span class="next"><a href="section2.html">Section 2</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
